{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrackerClass\u001b[39;00m \u001b[39mimport\u001b[39;00m handTracker\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/mit/EMG_CameraTracking/trackerClass.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m/home/haptix/haptix/haptix_controller/handsim/src/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from trackerClass import handTracker\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = handTracker(arm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LUKE arm\n",
    "if tracker.arm:\n",
    "    print('Initializing sensor readings...')\n",
    "    tracker.arm.initSensors()\n",
    "    print('Sensors initialized.')\n",
    "\n",
    "    tracker.arm.startup()\n",
    "    tracker.arm.shortModeSwitch(1)\n",
    "    print('Arm started. Starting tracking.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tracker\n",
    "print('Starting hand tracker')\n",
    "tracker.runTracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data saving\n",
    "tracker.saveData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plot the data\n",
    "fileName = 'handTrackingCoordinates_1670018641.1442099.csv'\n",
    "loadedData = pd.read_csv('./data/' + fileName)\n",
    "columnNames = [\"thumbPPos\", \"thumbYPos\", \"indexPos\", \"mrpPos\", \"wristRot\", \"wristFlex\", \"humPos\", \"elbowPos\"]\n",
    "positions = loadedData[columnNames].to_numpy()\n",
    "\n",
    "# plt.figure()\n",
    "for i in range(positions.shape[1]):\n",
    "    plt.plot(positions[:, i], label=columnNames[i])\n",
    "\n",
    "plt.title(fileName)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Angle (deg)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input video information: 2365 frames (30.0 FPS) captured at (1080, 1920)\n",
      "Processed frame 0 of 2365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame 100 of 2365\n",
      "Processed frame 200 of 2365\n",
      "Processed frame 300 of 2365\n",
      "Processed frame 400 of 2365\n",
      "Processed frame 500 of 2365\n",
      "Processed frame 600 of 2365\n",
      "Processed frame 700 of 2365\n",
      "Processed frame 800 of 2365\n",
      "Processed frame 900 of 2365\n",
      "Processed frame 1000 of 2365\n",
      "Processed frame 1100 of 2365\n",
      "Processed frame 1200 of 2365\n",
      "Processed frame 1300 of 2365\n",
      "Processed frame 1400 of 2365\n",
      "Processed frame 1500 of 2365\n",
      "Processed frame 1700 of 2365\n",
      "Processed frame 1800 of 2365\n",
      "Processed frame 1900 of 2365\n",
      "Processed frame 2000 of 2365\n",
      "Processed frame 2100 of 2365\n",
      "Processed frame 2200 of 2365\n",
      "Processed frame 2300 of 2365\n"
     ]
    }
   ],
   "source": [
    "# for running off saved video\n",
    "tracker.analyzeVideo(fileName='Slo-Mo Tracking.MOV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old Code Below\n",
    "- for running in ipynb, rather than through the arm tracking and control class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For static images:\n",
    "inputVideoTitle = 'Slo-Mo Tracking'\n",
    "vidFile = cv2.VideoCapture(inputVideoTitle + '.MOV')\n",
    "length = int(vidFile.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frameRate = 240 # np.ceil(vidFile.get(cv2.CAP_PROP_FPS))\n",
    "resolution = (int(vidFile.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vidFile.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "print(f'Input video information: {length} frames ({frameRate} FPS) captured at {resolution}')\n",
    "\n",
    "# get the column names for the hand landmarks\n",
    "lmkNames = [\"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \"THUMB_TIP\", \"INDEX_FINGER_MCP\", \"INDEX_FINGER_PIP\", \"INDEX_FINGER_DIP\", \"INDEX_FINGER_TIP\", \"MIDDLE_FINGER_MCP\", \"MIDDLE_FINGER_PIP\", \"MIDDLE_FINGER_DIP\", \"MIDDLE_FINGER_TIP\", \"RING_FINGER_MCP\", \"RING_FINGER_PIP\", \"RING_FINGER_DIP\", \"RING_FINGER_TIP\", \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\"]\n",
    "colNames = sum(list(map(lambda lmk: [lmk + \"_X\", lmk + \"_Y\", lmk + \"_Z\"], lmkNames)), []) # this is a stupid way to get X, Y, Z added to the names and collapse into a list\n",
    "\n",
    "# set up the saving data structure\n",
    "outputNp = np.empty((1, len(colNames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output video setup\n",
    "outputVid = cv2.VideoWriter(inputVideoTitle + '_annotated.avi', cv2.VideoWriter_fourcc(*'MPEG'), frameRate, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCoordinates(image, hand_landmarks):\n",
    "    rows, cols, _ = image.shape\n",
    "    grouped = [[lmk.x*cols, lmk.y*rows, lmk.z*cols] for lmk in hand_landmarks.landmark]\n",
    "    return np.concatenate(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    for frameNum in range(length):\n",
    "        status, frame = vidFile.read()\n",
    "\n",
    "        if not status:\n",
    "            print(f'Frame number {frameNum} not read correctly')\n",
    "            continue\n",
    "\n",
    "        # Read an image, flip it around y-axis for correct handedness output (see above).\n",
    "        image = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the BGR image to RGB before processing.\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Print handedness and draw hand landmarks on the image.\n",
    "        # print('Handedness:', results.multi_handedness)\n",
    "        if not results.multi_hand_landmarks:\n",
    "            continue\n",
    "\n",
    "        image_height, image_width, _ = image.shape\n",
    "        annotated_image = image.copy()\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # print('hand_landmarks:', hand_landmarks)\n",
    "            # print(\n",
    "            #     f'Index finger tip coordinates: (',\n",
    "            #     f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "            #     f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "            # )\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            landmarks = extractCoordinates(image, hand_landmarks)\n",
    "            outputNp = np.concatenate((outputNp, landmarks[None, :]), axis=0)\n",
    "\n",
    "        outputVid.write(cv2.flip(annotated_image, 1))\n",
    "\n",
    "        # cv2.imshow('images', cv2.flip(annotated_image, 1))\n",
    "\n",
    "        # Draw hand world landmarks.\n",
    "        if not results.multi_hand_world_landmarks:\n",
    "            continue\n",
    "\n",
    "        if not frameNum % 100:\n",
    "            print(f'Processed frame {frameNum} of {length}')\n",
    "\n",
    "        # for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "        #     mp_drawing.plot_landmarks(\n",
    "        #         hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "outputDf = pd.DataFrame(data=outputNp, columns=colNames)\n",
    "outputDf.to_csv(inputVideoTitle + '_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79ce7e6187e070a9a9a54be529f4ed3a0ba5f87731184520c540b2c56f30a41d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
